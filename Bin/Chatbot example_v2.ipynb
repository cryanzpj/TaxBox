{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot - example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from Chatbot Magazine - https://chatbotsmagazine.com/contextual-chat-bots-with-tensorflow-4391749d0077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a json file for intents (intents.json)\n",
    "- tags are the name of the intent  \n",
    "- patterns are the questions for the intent so there is a sentence patterns for the NN classifier of which intents  \n",
    "- resposnes are the answers to the question in that intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import intents file\n",
    "import json\n",
    "\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess data - words, intents classes and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hmoon/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use nltk work_tokenize\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 documents\n",
      "6 classes ['affirm', 'goodbye', 'greeting', 'taxcode', 'taxdescription', 'thanks']\n",
      "48 unique stemmed words [\"'s\", 'amount', 'anyon', 'ar', 'bye', 'cas', 'circumst', 'cod', 'correct', 'day', 'deduc', 'exceiv', 'good', 'goodby', 'hello', 'help', 'hi', 'how', 'in', 'indee', 'is', 'lat', 'law', 'liabl', 'limit', 'maxim', 'minim', 'of', 'ok', 'part', 'right', 'rul', 'sect', 'see', 'tax', 'thank', 'that', 'the', 'ther', 'und', 'what', 'wher', 'which', 'withdraw', 'ye', 'yeah', 'you', 'yup']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "        # add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train with BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a) Tranformation for TensorFlow: from documents of words into tensors of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test lists\n",
    "train_x = list(training[:,0])  # first column = BOW rep\n",
    "train_y = list(training[:,1])  # second column = intent class rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4999  | total loss: \u001b[1m\u001b[32m0.02297\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1000 | loss: 0.02297 - acc: 1.0000 -- iter: 32/40\n",
      "Training Step: 5000  | total loss: \u001b[1m\u001b[32m0.02234\u001b[0m\u001b[0m | time: 0.013s\n",
      "| Adam | epoch: 1000 | loss: 0.02234 - acc: 1.0000 -- iter: 40/40\n",
      "--\n",
      "INFO:tensorflow:/Users/yuhzhao/Desktop/hackathon/project-python-flask-webapp/Bin/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all of our data structures\n",
    "import pickle\n",
    "\n",
    "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Chatbot Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore all of our data structures\n",
    "import pickle\n",
    "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "train_x = data['train_x']\n",
    "train_y = data['train_y']\n",
    "\n",
    "# import our chat-bot intents file\n",
    "import json\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/yuhzhao/Desktop/hackathon/project-python-flask-webapp/Bin/model.tflearn\n"
     ]
    }
   ],
   "source": [
    "# load our saved model\n",
    "model.load('./model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for tokenizing and BOW vector rep of the query\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "p = bow(\"what is the tax code for qualified contribution limits?\", words)\n",
    "print (p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to classify intent and output responses based on the intent\n",
    "\n",
    "ERROR_THRESHOLD = 0.25\n",
    "\n",
    "def classify(sentence):\n",
    "    # generate probabilities from the model\n",
    "    results = model.predict([bow(sentence, words)])[0]\n",
    "    # filter out predictions below a threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return return_list\n",
    "\n",
    "def response(sentence, userID='123', show_details=False):\n",
    "    results = classify(sentence)\n",
    "    # if we have a classification then find the matching intent tag\n",
    "    if results:\n",
    "        # loop as long as there are matches to process\n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                # find a tag matching the first result\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    # a random response from the intent\n",
    "                    #return print(random.choice(i['responses']))\n",
    "                    return random.choice(i['responses'])\n",
    "\n",
    "            results.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('taxcode', 0.90338826)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1:\n",
    "classify('what is the tax code for qualified contribution limits?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tax code is'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1:\n",
    "response('what is the tax code for qualified contribution limits?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tax code is\n"
     ]
    }
   ],
   "source": [
    "r = response('what is the tax code for qualified contribution limits?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('taxdescription', 0.79001725)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2:\n",
    "classify('How does increased compensation from previous year taxed?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, thanks for visiting'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2:\n",
    "response('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import script to search the right tax code and description from tax corpus\n",
    "from taxcode_tfidf_search_script import *\n",
    "\n",
    "def full_response(sentence, top_n):\n",
    "    answer = response(sentence)\n",
    "    \n",
    "    if answer == \"Tax code is\":\n",
    "        temp = query_wrapper(sentence,cosine_sim_threshold=0.2,top=top_n)\n",
    "        if len(temp) == 0:\n",
    "            final = \"No tax code found\"\n",
    "        else:\n",
    "            temp_values = temp['title'].values\n",
    "            final = (answer + \" \" + temp_values).tolist()\n",
    "\n",
    "    elif answer == \"Here is what we found in the tax code:\":\n",
    "        temp = query_wrapper(sentence,cosine_sim_threshold=0.2,top=top_n)\n",
    "        if len(temp) == 0:\n",
    "            final = \"We have not found any section in tax code related to your question\"\n",
    "        else:\n",
    "            temp_values = temp['title'].values + \" \" + temp['text'].values\n",
    "            final = (answer + \" \" + temp_values).tolist()\n",
    "        \n",
    "    else:\n",
    "        final = answer\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tax code is 26 U.S. Code § 414 - Definitions and special rules',\n",
       " 'Tax code is 26 U.S. Code § 408A - Roth IRAs',\n",
       " 'Tax code is 26 U.S. Code § 402A - Optional treatment of elective deferrals as Roth contributions',\n",
       " 'Tax code is 26 U.S. Code § 414 - Definitions and special rules']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: tax codes top 5\n",
    "full_response(\"what is the tax code for qualified contribution limits?\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here is what we found in the tax code: 26 U.S. Code § 411 - Minimum vesting standards (D) Accrual for service before effective dateSubparagraphs (A)  (B)  and (C) shall not apply with respect to years of participation before the first plan year to which this section applies  but a defined benefit plan satisfies the requirements of this subparagraph with respect to such years of participation only if the accrued benefit of any participant with respect to such years of participation is not less than the greater of—(i)his accrued benefit determined under the plan  as in effect from time to time prior to September 2  1974  or(ii)an accrued benefit which is not less than one-half of the accrued benefit to which such participant would have been entitled if subparagraph (A)  (B)  or (C) applied with respect to such years of participation.',\n",
       " 'Here is what we found in the tax code: 26 U.S. Code § 410 - Minimum participation standards §\\u202f410.Minimum participation standards(a) Participation(1) Minimum age and service conditions(A) General ruleA trust shall not constitute a qualified trust under section 401(a) if the plan of which it is a part requires  as a condition of participation in the plan  that an employee complete a period of service with the employer or employers maintaining the plan extending beyond the later of the following dates—(i)the date on which the employee attains the age of 21; or(ii)the date on which he completes 1 year of service.',\n",
       " 'Here is what we found in the tax code: 26 U.S. Code § 410 - Minimum participation standards (4) Exclusion of employees not meeting age and service requirements(A) In generalIf a plan—(i)prescribes minimum age and service requirements as a condition of participation  and(ii)excludes all employees not meeting such requirements from participation then such employees shall be excluded from consideration for purposes of this subsection.']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: tax descriptions top 3\n",
    "full_response(\"For a benefit plan to be considered as a qualified plan, what are the minimum plan participation criteria?\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
